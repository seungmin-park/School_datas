# AI 프로그래밍 중간고사 정리

### 인공지능

인간이 가진 지적 능력을 컴퓨터로 구현하는 기술

### 머신 러닝

데이터와 알고리즘을 통해 컴퓨터를 학습시켜 인공지능의 성능을 향상 시키는 기술

### 딥 러닝

신경망을 이용한 머신 러닝 방법, 여러 층으로 이루어진 신경망을 사용한다.

우리의 신경계의 기본 단위인 뉴련을 모델화

인공 뉴련을 퍼셉트론(Perseptron)이라고 부름

하나의 인공 뉴런은 다 수의 입력을 받아 하나의 신호를 출력한다

![image](https://user-images.githubusercontent.com/78605779/163714158-936c211c-7402-4b58-bd5c-94c925093d55.png)

[이미지 출처](https://m.blog.naver.com/pwj6971/221614497987)

머신 러닝은 대부분 필요한 라이브러리를 선택하여 라이브러리내에 구현된 함수들을 조합하여 사용한다.

**대표적인 라이브러리**

![image](https://user-images.githubusercontent.com/78605779/163714269-4d9d6793-653d-4390-a1c8-905e8a4a5b28.png)

```
수업 시간에는 머신 러닝중에서 지도 학습을 주로 학습한다.
지도 학습 내에서도 회귀에 관하여 학습
```

학습 데이터 입력X와 출력Y를 통하여 결과를 예측하는 함수(y = f(x))를 도출해 낸다.

**특정 라이브러리 사용X, 경사 하강법 구현**

```py
import numpy as np
import matplotlib.pyplot as plt

X = np.array([5.0, 7.0, 12.0 , 13.0 , 19.0])
y = np.array([12.0, 19.0, 28.0 , 37.0 , 48.0])
w = 0       # 기울기
b = 0       # 절편

lrate = 0.001  # 학습률
epochs = 1000  # 반복 횟수

n = float(len(X)) # 입력 데이터의 개수

# 경사 하강법
for i in range(epochs):
    y_pred = w*X + b  			# 선형 회귀 예측값
    dw = (2/n) * sum(X * (y_pred-y)) 	# 넘파이 배열간의 산술 계산은 요소별로 적용
    db = (2/n) * sum(y_pred-y)  	# sum()은 모든 요소들의 합을 계산하는 내장 함수
    w = w - lrate * dw  		# 기울기 수정
    b = b - lrate * db  		# 절편 수정

# 기울기와 절편을 출력한다.
print("기울기 : ",w)
print("절편 : ",b)

# 예측값을 만든다.
y_pred = w*X + b

# 입력 데이터를 그래프 상에 찍는다.
plt.scatter(X, y)

print (w * 10 + b)

# 예측값은 선그래프로 그린다.
plt.plot([min(X), max(X)], [min(y_pred), max(y_pred)], color='red')
plt.show()
```

**결과**

![image](https://user-images.githubusercontent.com/78605779/163714470-ea205092-1c53-4829-8412-a8685753a7b6.png)

**sklearn 라이브러리 선형 회귀 모델 사용**

```py
import matplotlib.pylab as plt
import numpy as np
from sklearn import linear_model

# 선형 회귀 모델을 생성한다.
reg = linear_model.LinearRegression()

# 데이터는 파이썬의 리스트로 만들어도 되고 아니면 넘파이의 배열로 만들어도 됨
X = [[5], [7], [12], [13], [19]]# 반드시 2차원으로 만들어야 함
y = [12, 19, 28, 37, 48]

# 학습을 시킨다.
reg.fit(X, y)

print(reg.coef_)		# 직선의 기울기
print(reg.intercept_) 	# 직선의 y-절편
print(reg.score(X, y))

print(reg.predict([[10]]))

# 학습 데이터와 y 값을 산포도로 그린다.
plt.scatter(X, y, color='black')

# 학습 데이터를 입력으로 하여 예측값을 계산한다.
y_pred = reg.predict(X)

# 학습 데이터와 예측값으로 선그래프로 그린다.
# 계산된 기울기와 y 절편을 가지는 직선이 그려진다.
plt.plot(X, y_pred, color='blue', linewidth=3)
plt.show()
```

```
sklearn에서 X값은 2차원 배열이여야 한다.

X값을 1차원으로 했을 경우

ValueError: Expected 2D array, got 1D array instead:
array=[ 30  50  80  90 120].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
```

### 오차

실제 값과 예측값의 차이로 0에 가까울수록 정확도가 높은 것을 의미

#### 손실 함수

실제 값과 예측 값의 차이를 수치화한 함수

#### 평균 제곱 오차

머신 러닝이나 영상 처리에서 예측 값의 정확도를 측정하는 방법중 하나로 회귀에서 자주 사용하는 손실 함수이다.

![image](https://user-images.githubusercontent.com/78605779/163717081-0567c14a-c01c-4c55-bc06-82a1387f07c9.png)

#### 경사 하강법

손실 함수의 최소 값 지점을 찾아 나가는 방법

손실 함수를 미분하여 특정 구간의 순간 기울기를 구하여 기울기의 반대 방형으로 이동하면서 0이 되는 구간을 찾을 때 까지 반복한다

```
손실 함수 : y = (x-3)^2 +10

기울기(gradient) : y` = 2x - 6

학습률 0.2

초기 값 x = 10 가정

y` = 14

y` * 학습률  = 14 * 0.2 = 2.8

x - 2.8 = 7.2

두번 째 x = 7.2

...

y` = 0 이 될 때 까지 반복
```

학습률에 따라 최소값에 도달하는 시간관 정확도가 정해진다.

![image](https://user-images.githubusercontent.com/78605779/163721026-23a66d30-c0f2-440b-af1a-bf3c8e732072.png)

### 퍼셉트론(Perseptron)

다수의 입력을 받아 하나의 신호를 출력

가중치(w) : 입력 신호가 출력의 미치는 중요도

바이어스(b) : 뉴련이 얼마나 쉽게 활성화 되는지 결정

가중합 : `W1 * X1 + W2 * X2 + b`

가중합의 결과를 활성화 함수에 대입하여 최종 출력

![image](https://user-images.githubusercontent.com/78605779/163723488-cef1b0a2-66aa-4874-8170-857bc08a9b91.png)

#### 활성화 함수

- Step

  가중합이 0보다 작거나 같으면 0, 0보다 크면 1 출력

![image](https://user-images.githubusercontent.com/78605779/163723549-ec5fa537-7ab6-4e36-9826-75a9eac7f1b2.png)

- ReLU

  가중합이 0보다 작거나 같으면 0, 0보다 크면 가중합 그대로 출력

![image](https://user-images.githubusercontent.com/78605779/163723567-51cd7973-0645-4ca4-8872-c5c326ffc168.png)

- Sigmoid

![image](https://user-images.githubusercontent.com/78605779/163723561-5189ee02-64e4-4dde-83b2-a01ad4269af8.png)

- TanH

![image](https://user-images.githubusercontent.com/78605779/163723580-421cada9-c189-4ca0-bd5d-51787d135e95.png)

### numpy

**기본적인 명령어**
| 명령어 | 설명 |
| :-------: | :----: |
| ndim | 축의 개수 또는 차원의 개수 2차원 배열일 경우 2, 1차원 배열일 경우 1 |
| shape | 배열의 형상을 정수의 튜플 형태로 출력한다. n행 m열 -> (n,m) |
| szie | 배열 안에 존재하는 요소의 총 개수 |
| dtype | 배열 안에 존재하는 데이터 타입 ex)int64 |
| itemsize | 배열 요소의 데이터 크기(단위 : 바이트) int64 -> 8 |
| data | 배열의 메모리 주소 |
| reshape | 배열을 새롭게 지정하는 형상으로 변환|
| ones | 지정한 배열의 형상을 모든 요소를 1로 하여 생성|
| zeros |지정한 배열의 형상을 모든 요소를 1로 하여 생성 |
| arange |[start, ] stop, [step, ] dtype=None, start부터 stop까지 step 간격으로 배열 생성 stop 필수 파라미터, start = 0, step = 1 default 값 |
| concatenate | 지정한 두 배열을 합친다.|
| vstack | 두 배열을 세로로 합친다. |

```py
import numpy as np # numpy 선언

a = np.array([1,2,3,4,5]) # 1차원 배열 선언
# 출력 결과 -> array([1,2,3,4,5])

print(a[0]) # -> 1 배열의 특정 인덱스 값 출력
print(a.ndim) # -> 1
print(a.shape) # -> (5,)
print(a.size) # -> 5
print(a.dtype) # -> int64
print(a.itemsize) # -> 8

b = np.array([[1,2,3],[4,5,6],[7,8,9]]) #2차원 배열 선언
#array([[1,2,3],
#       [4,5,6],
#       [7,8,9]])

print(b[0]) #-> [1 2 3]
print(b[1][2]) # -> 6
print(b.ndim) # -> 2
print(b.shape) # -> (3,3)
print(b.size) # -> 9
print(b.dtype) # -> int64
print(b.itemsize) # -> 8
print(b.reshape.(1,-1).sahpe)# ->(1,9)

print(np.zeros((5,))) # [0,0,0,0,0]
print(np.zeros((5,5))) # [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]

print(np,ones((3,))) # [1,1,1]
print(np.ones((3,2))) # [[1,1],[1,1],[1,1]]

c = np.array([6,7,8,9,10])
d = np.array([[10,11,12],[13,14,15],[16,17,18]])

print(np.concatenate((a,c), axis=0)) #[1,2,3,4,5,6,7,8,8,9,10]
print(np.vstack((a,c))) #[[1,2,3,4,5],[6,7,8,9,10]]

print(np.concatenate((b,d),axis=1)) #[[1,2,3,10,11,12],[4,5,6,13,14,15],[7,8,9,16,17,18]]
print(np.concatenate((b,d),axis=0))#[[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15],[16,17,18]]
print(np.vstack((b,d)))#[[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15],[16,17,18]]

print(np.arange(10)) # [0,1,2,3,4,5,6,7,8,9]
print(np.arange(3,10)) # [3,4,5,6,7,8,9]
print(np.arange(3,10,2)) # [3,5,7,9]

print(c[:,np.newaxis].shape) #(5,1)
print(c[:,np.newaxis]) #[[6],[7],[8],[9],[10]]
print(c[np.newaxis,:].shape) #(1,5)
print(c[np.newaxis,:]) # [[6,7,8,9,10]]

print(c.T) # [6 7 8 9 10] 변화 없음
print(d.T) #[[10,13,16],[11,14,17],[12,15,18]]

print(c.flatten()) # [6,7,8,9,10]
print(d.flatten()) # [10,11,12,13,14,15,16,17,18]
```

```
다층 퍼셉트론 ppt 참고하기
```

[참고 문헌 - 딥 러닝 express](http://www.yes24.com/Product/Goods/102727115)
